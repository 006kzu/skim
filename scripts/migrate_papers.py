import sys
import os
import time

# Add parent dir to path to import database.py
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database import get_client, PAPER_COLUMNS
from supabase import create_client
from dotenv import load_dotenv

def main():
    # 1. Connect to Source (Local/Dev)
    # Uses .env implicitly loaded by database.py
    print("--- MIGRATING PAPERS ---")
    print("1. Connecting to Source Database (from .env)...")
    source_client = get_client() # Uses env vars
    
    if not source_client:
        print("❌ Could not connect to Source DB. Check .env")
        return

    # 2. Fetch All Papers
    print("2. Fetching all papers from Source...")
    try:
        # Fetch everything. Pagination might be needed for huge DBs, but likely fine for <1000
        res = source_client.table("papers").select("*").execute()
        papers = res.data
        print(f"   found {len(papers)} papers.")
    except Exception as e:
        print(f"❌ Error fetching source papers: {e}")
        return

    if not papers:
        print("   No papers to migrate.")
        return

    # 3. Connect to Destination
    dest_url = os.environ.get("DEST_URL")
    dest_key = os.environ.get("DEST_KEY")

    if not dest_url or not dest_key:
        print("\n❌ MISSING DESTINATION CREDENTIALS")
        print("Please run the script providing the PRODUCTION credentials:")
        print("export DEST_URL='https://your-prod-db.supabase.co'")
        print("export DEST_KEY='your-service-role-key'")
        print("python3 scripts/migrate_papers.py")
        return

    print(f"\n3. Connecting to Destination: {dest_url}...")
    try:
        dest_client = create_client(dest_url, dest_key)
    except Exception as e:
        print(f"❌ Error connecting to Destination: {e}")
        return

    # 4. Insert Data
    print(f"4. Migrating {len(papers)} papers...")
    
    success_count = 0
    skip_count = 0
    
    # Process in batches of 50
    batch_size = 50
    for i in range(0, len(papers), batch_size):
        batch = papers[i:i+batch_size]
        
        # Clean batch: remove 'id' to let Prod allow it (or keep it if we want UUID parity)
        # Usually better to KEEP 'id' to maintain references if we migrate comments later.
        # But if Prod has different IDs, we break foreign keys.
        # Strategy: Try to insert AS IS. If ID conflict, we upscale/upsert.
        
        # However, checking duplicates by URL/Title is safer if IDs match by coincidence?
        # Supabase 'upsert' works on Primary Key.
        
        try:
            # Upsert ensures that if ID exists, we update it.
            # But wait, if IDs are UUIDs generated by Supabase, collisions are rare UNLESS we are copying the same data.
            dest_client.table("papers").upsert(batch).execute()
            success_count += len(batch)
            print(f"   Batch {i//batch_size + 1} uploaded ({len(batch)} items)")
        except Exception as e:
            print(f"   ⚠️ Batch failed: {e}")
            # Try 1-by-1 fallback
            for p in batch:
                try:
                    dest_client.table("papers").upsert(p).execute()
                    success_count += 1
                except Exception as inner_e:
                    print(f"      Skipped '{p.get('title')[:20]}...': {inner_e}")
                    skip_count += 1

    print(f"\n✅ MIGRATION COMPLETE")
    print(f"   Transferred: {success_count}")
    print(f"   Skipped/Failed: {skip_count}")

if __name__ == "__main__":
    main()
